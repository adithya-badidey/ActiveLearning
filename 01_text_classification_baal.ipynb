{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KM6I2WSiBmMu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import baal\n",
    "from baal import ActiveLearningDataset\n",
    "from baal.active import get_heuristic, ActiveLearningLoop\n",
    "from baal.bayesian.dropout import MCDropoutModule\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pathlib\n",
    "import os\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch: \t\t 1.13.0+cu116\n",
      "GPU:         \t\t NVIDIA A100 80GB PCIe MIG 2g.20gb\n",
      "Memory Usage:\t 0.0 GB /  0.0 GB\n"
     ]
    }
   ],
   "source": [
    "debug=False\n",
    "\n",
    "print(\"Pytorch: \\t\\t\", torch.__version__)\n",
    "if not debug and torch.cuda.is_available():\n",
    "    device='cuda'\n",
    "    print('GPU:         \\t\\t', torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:\\t',\n",
    "        round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB / ',\n",
    "        round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"GPU is **not available**\")\n",
    "    device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OMYkCTgRWBxe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Python Path\n",
      "Project Root Path: /home/default/workspace\n",
      "Project Source Root Path: /home/default/workspace/ActiveLearning\n",
      "Project Data Path: /home/default/workspace/ActiveLearning/data\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# To be able to reference packages/modules in this repository, this\n",
    "# relative path must be added to the python path. Your notebook may be \n",
    "# in a different folder, so modify this variable to point to the src \n",
    "# folder.\n",
    "proj_notebooks_root = pathlib.Path().absolute()\n",
    "proj_root_path = proj_notebooks_root.parent\n",
    "data_path = proj_notebooks_root / \"data\"\n",
    "\n",
    "if proj_root_path not in sys.path:\n",
    "    sys.path.insert(0, proj_root_path.as_posix())\n",
    "    print(\"Updated Python Path\")\n",
    "\n",
    "print(f\"Project Root Path: {proj_root_path}\")\n",
    "print(f\"Project Source Root Path: {proj_notebooks_root}\")\n",
    "print(f\"Project Data Path: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the 'aclImdb_v1' dataset.\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path('data/aclImdb_v1/')\n",
    "\n",
    "if data_dir.exists():\n",
    "    print(\"Found the 'aclImdb_v1' dataset.\")\n",
    "else:\n",
    "    print(\"Downloading the 'aclImdb_v1' dataset.\")\n",
    "    dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    !wget {dataset_url} -P {data_dir.parent}\n",
    "    !mkdir {data_dir}\n",
    "    !tar xf {data_dir.parent / \"aclImdb_v1.tar.gz\"} -C {data_dir}\n",
    "    !mv {data_dir / 'aclImdb' / 'train' / 'unsup'} {data_dir / 'unsup'}\n",
    "# data_dir = data_dir / 'aclImdb'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XMuIz-pmFDU3"
   },
   "outputs": [],
   "source": [
    "def cleanup(data):\n",
    "    return data.replace('<br />', '')\n",
    "\n",
    "class ArrayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, array):\n",
    "        self.array = array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        res = self.array[index]\n",
    "        return cleanup(open(res[0]).read()), res[1]\n",
    "    \n",
    "    def split(self, p=0.5):\n",
    "        count = len(self.array)\n",
    "        index = np.arange(count)\n",
    "        first = int(count * p)\n",
    "        return [\n",
    "            ArrayDataset(self.array[index[:first]], \n",
    "                    image_transforms=self.image_transforms), \n",
    "            ArrayDataset(self.array[index[first:]], \n",
    "                    image_transforms=self.image_transforms)\n",
    "        ]\n",
    "    \n",
    "    def split_count(self, first):\n",
    "        return [\n",
    "            ArrayDataset(self.array[:first]), \n",
    "            ArrayDataset(self.array[first:])\n",
    "        ]\n",
    "\n",
    "def get_datasets(initial_pool, path):\n",
    "#     test_dir = data_dir / 'test'\n",
    "    train_dir = path / 'train'\n",
    "    print(train_dir)\n",
    "    files = [y for y in (train_dir).glob('*/*')] \n",
    "    data = np.array([(i, i.parent.stem) for id, i in enumerate(files)])\n",
    "    print(len(data), \"examples found\")\n",
    "\n",
    "    dataset = ArrayDataset(data)\n",
    "\n",
    "    test_set, active_set = dataset.split_count(500)\n",
    "    print(\"Active Set: \", len(active_set))\n",
    "    print(\"Test Set: \", len(test_set))\n",
    "    \n",
    "    active_set = ActiveLearningDataset(active_set)\n",
    "    \n",
    "    active_set.label_randomly(initial_pool)\n",
    "    return active_set, test_set\n",
    "\n",
    "# get_datasets(100, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "\n",
    "def collate_unlabelled_batch(text_pipeline, batch):\n",
    "     indices_list, text_list = [], []\n",
    "     for (text, index) in batch:\n",
    "          input = text_pipeline(text)\n",
    "          indices_list.append(index)\n",
    "          \n",
    "          a = torch.tensor(input[:sequence_max_length], dtype=torch.int64)\n",
    "          b = torch.zeros(max(0, sequence_max_length - len(input)), dtype=torch.int64)\n",
    "          text_list.append(torch.cat((a,b)))\n",
    "               \n",
    "     text_list = torch.stack(text_list)\n",
    "     return text_list, indices_list\n",
    "    \n",
    "\n",
    "def collate_batch(text_pipeline, label_pipeline, batch):\n",
    "     label_list, text_list = [], []\n",
    "     for (text, label) in batch:\n",
    "          input = text_pipeline(text)\n",
    "          label_list.append(torch.tensor([label_pipeline(label)], dtype=torch.float32))\n",
    "          \n",
    "          a = torch.tensor(input[:sequence_max_length], dtype=torch.int64)\n",
    "          b = torch.zeros(max(0, sequence_max_length - len(input)), dtype=torch.int64)\n",
    "          text_list.append(torch.cat((a,b)))\n",
    "               \n",
    "     label_list = torch.stack(label_list)\n",
    "     text_list = torch.stack(text_list)\n",
    "     return text_list, label_list\n",
    "\n",
    "class ClassifyNet(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        # self.pool = nn.AvgPool1d(3, stride=2)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        # print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.mean(x, 1)\n",
    "        x = self.dropout2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, history=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss, accuracy = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        #FORWARD PASS\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        accuracy += (torch.round(pred) == y).type(torch.float).sum().item()\n",
    "\n",
    "        # if batch % 100 == 0:\n",
    "        #     loss, current = loss.item(), batch * len(X)\n",
    "        #     print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    accuracy /= size\n",
    "    total_loss /= size\n",
    "    if history is not None:\n",
    "        history['train_loss'].append(total_loss)\n",
    "        history['train_accuracy'].append(accuracy)\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, history):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    loss, accuracy = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            \n",
    "            loss += loss_fn(pred, y).item()\n",
    "            accuracy += (torch.round(pred) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= num_batches\n",
    "    accuracy /= size\n",
    "\n",
    "    history['test_loss'].append(loss)\n",
    "    history['test_accuracy'].append(accuracy)\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model_wrapper, loss_fn, train_dataset, test_dataset,epochs=40):\n",
    "    history = {\n",
    "        'train_loss':[],\n",
    "        'train_accuracy':[],\n",
    "        'test_loss':[],\n",
    "        'test_accuracy':[]\n",
    "    }\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        batch_size=p_batch_size,\n",
    "        collate_fn=partial(collate_batch, text_pipeline, label_pipeline))\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        num_workers=8,\n",
    "        batch_size=p_batch_size, \n",
    "        collate_fn=partial(collate_batch, text_pipeline, label_pipeline))\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model_wrapper.model.parameters(), lr=4e-3)\n",
    "\n",
    "    for t in tqdm(range(epochs), bar_format=\"{elapsed} Elapsed | {percentage:3.0f}% done |{bar}| {n_fmt}/{total_fmt} [{remaining} remaining | {rate_fmt}{postfix}]\", unit=\"epoch\", total=epochs):\n",
    "        train_loop(train_dataloader, model_wrapper.model, loss_fn, optimizer, history)\n",
    "        test_loop(test_dataloader, model_wrapper.model, loss_fn, history)\n",
    "\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Cuda: True\n"
     ]
    }
   ],
   "source": [
    "p_active_learning_steps = 3\n",
    "\n",
    "p_initial_pool = 300\n",
    "p_query_size = 300\n",
    "p_query_interations = 20\n",
    "\n",
    "p_reduce=\"sum\"\n",
    "\n",
    "p_learning_epochs=30\n",
    "p_batch_size = 32\n",
    "p_learning_rate = 0.001\n",
    "\n",
    "p_classes = 1\n",
    "\n",
    "vocab_size = 10000\n",
    "emsize = 16\n",
    "sequence_max_length=500\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "print(\"Use Cuda:\", use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "def build_vocab_from_iterator(iterator, min_freq: int = 1, specials = [], special_first: bool = True, vocab_size = None):\n",
    "    counter = Counter()\n",
    "    for tokens in iterator:\n",
    "        counter.update(tokens)\n",
    "\n",
    "    if specials is not None:\n",
    "        for tok in specials:\n",
    "            del counter[tok]\n",
    "\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[0])\n",
    "    sorted_by_freq_tuples.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    if vocab_size is not None:\n",
    "        sorted_by_freq_tuples = sorted_by_freq_tuples[:vocab_size]\n",
    "\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "    if specials is not None:\n",
    "        if special_first:\n",
    "            specials = specials[::-1]\n",
    "        for symbol in specials:\n",
    "            ordered_dict.update({symbol: min_freq})\n",
    "            ordered_dict.move_to_end(symbol, last=not special_first)\n",
    "\n",
    "    word_vocab = torchtext.vocab.vocab(ordered_dict, min_freq=min_freq)\n",
    "    return word_vocab\n",
    "\n",
    "\n",
    "# We will use the FocalLoss\n",
    "if p_classes > 1:\n",
    "    criterion = FocalLoss(gamma=2, alpha=0.25)\n",
    "else:\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "model = ClassifyNet(vocab_size, emsize, p_classes)\n",
    "\n",
    "# This will enable Dropout at test time.\n",
    "model = MCDropoutModule(model)\n",
    "\n",
    "# Put everything on GPU.\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Keep a copy of the original weights\n",
    "initial_weights = deepcopy(model.state_dict())\n",
    "\n",
    "# Add metrics\n",
    "model = baal.ModelWrapper(model, \n",
    "                          criterion, \n",
    "                          replicate_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probabilities(pool, **kwargs):\n",
    "    return model.predict_on_dataset(pool, \n",
    "                                  collate_fn = partial(collate_batch, text_pipeline, label_pipeline), \n",
    "                                  **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Heuristic: random\n",
      "====================\n",
      "data/aclImdb_v1/train\n",
      "25000 examples found\n",
      "Active Set:  24500\n",
      "Test Set:  500\n",
      "Step: 1\n",
      "Training Model with 300 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:46 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.56s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T15:57:04.990178Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 75.22it/s] \n",
      "--------------------\n",
      "Step: 2\n",
      "Training Model with 600 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:49 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.66s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T15:57:55.802004Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 84.56it/s] \n",
      "--------------------\n",
      "Step: 3\n",
      "Training Model with 900 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:54 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.83s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T15:58:51.902469Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 82.64it/s] \n",
      "--------------------\n",
      "====================\n",
      "Heuristic: entropy\n",
      "====================\n",
      "data/aclImdb_v1/train\n",
      "25000 examples found\n",
      "Active Set:  24500\n",
      "Test Set:  500\n",
      "Step: 1\n",
      "Training Model with 300 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:46 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.56s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T15:59:42.297811Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 77.13it/s] \n",
      "--------------------\n",
      "Step: 2\n",
      "Training Model with 600 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:51 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.70s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T16:00:34.665043Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 85.44it/s] \n",
      "--------------------\n",
      "Step: 3\n",
      "Training Model with 900 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:56 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.87s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T16:01:31.918916Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 79.03it/s] \n",
      "--------------------\n",
      "====================\n",
      "Heuristic: bald\n",
      "====================\n",
      "data/aclImdb_v1/train\n",
      "25000 examples found\n",
      "Active Set:  24500\n",
      "Test Set:  500\n",
      "Step: 1\n",
      "Training Model with 300 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:47 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.57s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T16:02:22.709903Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 83.55it/s] \n",
      "--------------------\n",
      "Step: 2\n",
      "Training Model with 600 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:51 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.71s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T16:03:15.056456Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 77.61it/s] \n",
      "--------------------\n",
      "Step: 3\n",
      "Training Model with 900 examples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "00:54 Elapsed | 100% done |██████████| 30/30 [00:00 remaining |  1.82s/epoch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying...\n",
      "[1415865-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T16:04:10.971196Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m2000\u001b[0m\n",
      "100%|██████████| 63/63 [00:00<00:00, 83.15it/s] \n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "heuristics = ['random', 'entropy', 'bald']\n",
    "all_histories = []\n",
    "for h in heuristics:\n",
    "    print(\"=\"*20)\n",
    "    print(f\"Heuristic: {h}\")\n",
    "    print(\"=\"*20)\n",
    "    if h == 'batch_bald':\n",
    "        heuristic = get_heuristic(h, num_samples=1000)\n",
    "    else:\n",
    "        heuristic = get_heuristic(h)\n",
    "\n",
    "    active_set, test_set = get_datasets(p_initial_pool, data_dir)\n",
    "\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(active_set), specials=[\"<pad>\", \"<unk>\"], vocab_size=10000-2)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "    label_pipeline = lambda x: 1 if x == 'pos' else 0\n",
    "\n",
    "    # The ALLoop is in charge of predicting the uncertainty and\n",
    "    loop = ActiveLearningLoop(\n",
    "        active_set,\n",
    "        get_probabilities,\n",
    "        heuristic=heuristic,\n",
    "        query_size=p_query_size,\n",
    "        # Instead of predicting on the entire pool, only a subset is used\n",
    "        max_sample=2000,\n",
    "        batch_size=p_batch_size,\n",
    "        iterations=p_query_interations,\n",
    "        use_cuda=use_cuda,\n",
    "    )\n",
    "    history = []\n",
    "    all_histories.append(history)\n",
    "    for epoch in range(p_active_learning_steps):\n",
    "        print(f\"Step: {epoch + 1}\")\n",
    "        # Following Gal et al. 2016, we reset the weights.\n",
    "        model.load_state_dict(initial_weights)\n",
    "        # Train 50 epochs before sampling.\n",
    "\n",
    "        print(f\"Training Model with {len(active_set)} examples...\")\n",
    "        model, stats = train_model(model, criterion, \n",
    "                                   active_set, test_set, \n",
    "                                   epochs=p_learning_epochs)\n",
    "        history.append(stats)\n",
    "\n",
    "        print(\"Querying...\")\n",
    "\n",
    "        should_continue = loop.step()\n",
    "\n",
    "        print(\"-\"*20)\n",
    "\n",
    "        if not should_continue:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " {'train_loss': [0.023267762263615925,\n",
       "   0.023034343520800273,\n",
       "   0.022995293935139972,\n",
       "   0.022856536706288656,\n",
       "   0.022935722271601358,\n",
       "   0.0228342866897583,\n",
       "   0.022800857027371724,\n",
       "   0.02287408709526062,\n",
       "   0.02254206379254659,\n",
       "   0.022580652634302777,\n",
       "   0.022379831671714784,\n",
       "   0.02225578745206197,\n",
       "   0.02237004299958547,\n",
       "   0.02214676102002462,\n",
       "   0.02201523224512736,\n",
       "   0.021775400241216023,\n",
       "   0.021246384580930075,\n",
       "   0.021196983257929482,\n",
       "   0.02100347101688385,\n",
       "   0.020669724345207214,\n",
       "   0.02063179572423299,\n",
       "   0.019891916513442992,\n",
       "   0.019621530175209047,\n",
       "   0.01929427186648051,\n",
       "   0.01903814673423767,\n",
       "   0.018251535892486574,\n",
       "   0.017969027757644654,\n",
       "   0.017528752982616424,\n",
       "   0.016769630114237467,\n",
       "   0.016886346340179444],\n",
       "  'train_accuracy': [0.45,\n",
       "   0.55,\n",
       "   0.5333333333333333,\n",
       "   0.5466666666666666,\n",
       "   0.55,\n",
       "   0.5466666666666666,\n",
       "   0.5433333333333333,\n",
       "   0.5533333333333333,\n",
       "   0.57,\n",
       "   0.55,\n",
       "   0.5633333333333334,\n",
       "   0.5733333333333334,\n",
       "   0.54,\n",
       "   0.5766666666666667,\n",
       "   0.61,\n",
       "   0.62,\n",
       "   0.6666666666666666,\n",
       "   0.65,\n",
       "   0.6733333333333333,\n",
       "   0.7133333333333334,\n",
       "   0.7033333333333334,\n",
       "   0.79,\n",
       "   0.7866666666666666,\n",
       "   0.83,\n",
       "   0.8233333333333334,\n",
       "   0.84,\n",
       "   0.84,\n",
       "   0.8466666666666667,\n",
       "   0.88,\n",
       "   0.88],\n",
       "  'test_loss': [0.6662092991173267,\n",
       "   0.6237390264868736,\n",
       "   0.6257907785475254,\n",
       "   0.620707381516695,\n",
       "   0.6198303624987602,\n",
       "   0.654737077653408,\n",
       "   0.6522196605801582,\n",
       "   0.6457199603319168,\n",
       "   0.6336450800299644,\n",
       "   0.6288622058928013,\n",
       "   0.6172302514314651,\n",
       "   0.6108154691755772,\n",
       "   0.6054104007780552,\n",
       "   0.6233272217214108,\n",
       "   0.6115610972046852,\n",
       "   0.6179850548505783,\n",
       "   0.5996959991753101,\n",
       "   0.5758651830255985,\n",
       "   0.5789460279047489,\n",
       "   0.6099866554141045,\n",
       "   0.5905660092830658,\n",
       "   0.5779373161494732,\n",
       "   0.600386381149292,\n",
       "   0.6010937243700027,\n",
       "   0.579798761755228,\n",
       "   0.579535149037838,\n",
       "   0.5904432386159897,\n",
       "   0.5636510439217091,\n",
       "   0.5721593983471394,\n",
       "   0.5603206120431423],\n",
       "  'test_accuracy': [0.75,\n",
       "   0.964,\n",
       "   0.93,\n",
       "   0.97,\n",
       "   0.972,\n",
       "   0.806,\n",
       "   0.824,\n",
       "   0.834,\n",
       "   0.89,\n",
       "   0.906,\n",
       "   0.93,\n",
       "   0.934,\n",
       "   0.934,\n",
       "   0.86,\n",
       "   0.876,\n",
       "   0.842,\n",
       "   0.88,\n",
       "   0.926,\n",
       "   0.918,\n",
       "   0.814,\n",
       "   0.852,\n",
       "   0.87,\n",
       "   0.78,\n",
       "   0.758,\n",
       "   0.83,\n",
       "   0.806,\n",
       "   0.748,\n",
       "   0.822,\n",
       "   0.796,\n",
       "   0.796]})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_histories[-1]), all_histories[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 3,\n",
       " dict_keys(['train_loss', 'train_accuracy', 'test_loss', 'test_accuracy']))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_histories), len(all_histories[0]), all_histories[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "results_dir = pathlib.Path('results')\n",
    "\n",
    "timestamp, results_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(results_dir / f'{timestamp}-log.txt', 'w') as logfile:\n",
    "    logfile.write(json.dumps(all_histories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotHistory(history):\n",
    "#     metrics = ['train_loss', 'train_precision', 'train_recall', 'train_f1score',\n",
    "#            'test_loss','test_precision','test_recall', 'test_f1score']\n",
    "    metrics = list(history[0].keys())\n",
    "    gradients = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "                          'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
    "                          'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
    "    norm = matplotlib.colors.Normalize(vmin=0, vmax=len(history))\n",
    "    rows = 2\n",
    "    cols = round(len(metrics)/rows)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12,8), sharey = True)\n",
    "\n",
    "    for index, metric in enumerate(metrics):\n",
    "        cmap = matplotlib.cm.get_cmap(gradients[index])\n",
    "\n",
    "        ax = axes[index//cols,index%cols]\n",
    "        for i, val in enumerate(history):\n",
    "            ax.plot(val[metric], color=cmap(norm(i)))\n",
    "        ax.set_title(metric)\n",
    "\n",
    "print(\"For Heuristic: \", heuristics[0])\n",
    "plotHistory(all_histories[0])\n",
    "plt.savefig(results_dir / f'{timestamp}-{heuristics[0]}.png', bbox_inches='tight')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Heuristic: \", heuristics[1])\n",
    "plotHistory(all_histories[1])\n",
    "plt.savefig(results_dir / f'{timestamp}-{heuristics[1]}.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Heuristic: \", heuristics[2])\n",
    "plotHistory(all_histories[2])\n",
    "plt.savefig(results_dir / f'{timestamp}-{heuristics[2]}.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For Heuristic: \", heuristics[3])\n",
    "plotHistory(all_histories[3])\n",
    "plt.savefig(results_dir / f'{timestamp}-{heuristics[3]}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
