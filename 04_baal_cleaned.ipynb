{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KM6I2WSiBmMu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.13) or chardet (5.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import baal\n",
    "from baal import ActiveLearningDataset\n",
    "from baal.active import get_heuristic, ActiveLearningLoop\n",
    "from baal.bayesian.dropout import MCDropoutModule\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OMYkCTgRWBxe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Python Path\n",
      "Project Root Path: /home/default/workspace\n",
      "Project Source Root Path: /home/default/workspace/ActiveLearning\n",
      "Project Data Path: /home/default/workspace/ActiveLearning/data\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# To be able to reference packages/modules in this repository, this\n",
    "# relative path must be added to the python path. Your notebook may be \n",
    "# in a different folder, so modify this variable to point to the src \n",
    "# folder.\n",
    "proj_notebooks_root = pathlib.Path().absolute()\n",
    "proj_root_path = proj_notebooks_root.parent\n",
    "data_path = proj_notebooks_root / \"data\"\n",
    "\n",
    "if proj_root_path not in sys.path:\n",
    "    sys.path.insert(0, proj_root_path.as_posix())\n",
    "    print(\"Updated Python Path\")\n",
    "\n",
    "print(f\"Project Root Path: {proj_root_path}\")\n",
    "print(f\"Project Source Root Path: {proj_notebooks_root}\")\n",
    "print(f\"Project Data Path: {data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found dataset at  /home/default/workspace/ActiveLearning/data/ConglomerateConcreteCrackDataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "base_dir = data_path / 'ConglomerateConcreteCrackDataset'\n",
    "if not base_dir.exists():\n",
    "    dataset_url = 'https://data.lib.vt.edu/ndownloader/articles/16625056/versions/1'\n",
    "\n",
    "    ! wget {dataset_url} -P {data_path}\n",
    "    ! unzip -q {data_path / '1'} -d {data_path}\n",
    "    ! unzip -q {data_path / 'Conglomerate\\ Concrete\\ Crack\\ Detection.zip'} -d {data_path}\n",
    "    ! mv {data_path/'Conglomerate\\ Concrete\\ Crack\\ Detection'} {base_dir}\n",
    "    ! mv {data_path / 'README_congl_dataset.rtf'}  {data_path/'ConglomerateConcreteCrackDataset'}     \n",
    "    ! rm {data_path / 'Conglomerate\\ Concrete\\ Crack\\ Detection.zip'}\n",
    "    ! rm {data_path / '1'}\n",
    "else:\n",
    "    print(\"Found dataset at \", base_dir.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wEvOwdO3CQrM"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    References:\n",
    "        Author: clcarwin\n",
    "        Site https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.data.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            select = (target != 0).type(torch.LongTensor).to(self.alpha.device)\n",
    "            at = self.alpha.gather(0, select.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XMuIz-pmFDU3"
   },
   "outputs": [],
   "source": [
    "def mean_regions(n, grid_size=16):\n",
    "    # Compute the mean uncertainty per regions.\n",
    "    # [batch_size, W, H]\n",
    "    n = torch.from_numpy(n[:, None, ...])\n",
    "    # [Batch_size, 1, grid, grid]\n",
    "    out = F.adaptive_avg_pool2d(n, grid_size)\n",
    "    return np.mean(out.view([-1, grid_size**2]).numpy(), -1)\n",
    "\n",
    "\n",
    "class ArrayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, array, image_transforms=None, both_transforms=None):\n",
    "        self.array = array\n",
    "                \n",
    "        self.image_transforms = image_transforms\n",
    "        self.segment_transforms = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.array)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imagePath, segmentPath = self.array[index]\n",
    "        image = Image.open(imagePath).convert('RGB')\n",
    "        segment = Image.open(segmentPath).convert('L')\n",
    "        segment = self.segment_transforms(segment)        \n",
    "            \n",
    "        if self.image_transforms is not None:\n",
    "            image = self.image_transforms(image)\n",
    "        \n",
    "        # print(image.shape, segment.shape)\n",
    "\n",
    "        if image.shape != (3, 448, 448):\n",
    "            print(f\"Image shape is {image.shape}\")\n",
    "        if segment.shape != (1, 448, 448):\n",
    "            print(f\"Segment shape is {segment.shape}\")\n",
    "        \n",
    "        return image, segment.type(torch.int64) \n",
    "    \n",
    "    def split(self, p=0.5):\n",
    "        count = len(self.array)\n",
    "        index = np.arange(count)\n",
    "        first = int(count * p)\n",
    "        return [\n",
    "            ArrayDataset(self.array[index[:first]], \n",
    "                    image_transforms=self.image_transforms), \n",
    "            ArrayDataset(self.array[index[first:]], \n",
    "                    image_transforms=self.image_transforms)\n",
    "        ]\n",
    "\n",
    "def get_datasets(initial_pool, path):\n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    X_dir = path/'Train'/'images'\n",
    "    y_dir = path/'Train'/'masks'\n",
    "\n",
    "    files = [y for y in X_dir.glob('*')]\n",
    "\n",
    "    for i in files:\n",
    "        assert((y_dir / i.name).exists())\n",
    "\n",
    "    data = np.array([(i, (y_dir / i.name)) for id, i in enumerate(files)])\n",
    "\n",
    "    dataset = ArrayDataset(data, image_transforms = transform)\n",
    "\n",
    "\n",
    "    active_set, test_set = dataset.split(0.9)\n",
    "    print(\"Active Set: \", len(active_set))\n",
    "    print(\"Test Set: \", len(test_set))\n",
    "    \n",
    "    active_set = ActiveLearningDataset(active_set)\n",
    "    \n",
    "    active_set.label_randomly(initial_pool)\n",
    "    return active_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use Cuda: True\n",
      "Active Set:  8909\n",
      "Test Set:  990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 448, 448]), torch.Size([1, 448, 448]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_active_learning_steps = 3\n",
    "\n",
    "p_initial_pool = 60\n",
    "p_query_size = 60\n",
    "p_query_interations = 20\n",
    "\n",
    "p_heuristic = \"random\"\n",
    "p_reduce=\"sum\"\n",
    "\n",
    "p_leaning_epochs=30\n",
    "p_lr = 0.001\n",
    "p_batch_size = 16\n",
    "\n",
    "p_classes = 2\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "print(\"Use Cuda:\", use_cuda)\n",
    "\n",
    "active_set, test_set = get_datasets(p_initial_pool, base_dir)\n",
    "\n",
    "image, segment = active_set[0]\n",
    "image.shape, segment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We will use the FocalLoss\n",
    "if p_classes > 1:\n",
    "    criterion = FocalLoss(gamma=2, alpha=0.25)\n",
    "else:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# \n",
    "# model = smp.Unet(\n",
    "#     encoder_name=\"resnext50_32x4d\",\n",
    "#     encoder_depth=5,\n",
    "#     encoder_weights=\"imagenet\",\n",
    "#     decoder_use_batchnorm=False,\n",
    "#     classes=1,\n",
    "# )\n",
    "# add_dropout(model, classes=1, activation=None)\n",
    "\n",
    "model = smp.Unet('resnet34', \n",
    "                 classes=p_classes,\n",
    "                 encoder_weights=\"imagenet\",\n",
    "                 decoder_use_batchnorm=False)\n",
    "model.segmentation_head[1] = nn.Dropout2d(0.5)\n",
    "\n",
    "\n",
    "# This will enable Dropout at test time.\n",
    "model = MCDropoutModule(model)\n",
    "\n",
    "# Put everything on GPU.\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# Make an optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=p_lr, \n",
    "    momentum=0.9, \n",
    "    weight_decay=5e-4)\n",
    "\n",
    "# Keep a copy of the original weights\n",
    "initial_weights = deepcopy(model.state_dict())\n",
    "\n",
    "# Add metrics\n",
    "model = baal.ModelWrapper(model, criterion, replicate_in_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[802455-MainThread] [baal.modelwrapper:train_on_dataset:83] \u001b[2m2022-12-16T01:37:34.066460Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStarting training             \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m120\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m30\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:train_on_dataset:94] \u001b[2m2022-12-16T01:41:38.423057Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mTraining complete             \u001b[0m \u001b[36mtrain_loss\u001b[0m=\u001b[35m0.027370166033506393\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:test_on_dataset:123] \u001b[2m2022-12-16T01:41:38.434731Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStarting evaluating           \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m990\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:test_on_dataset:133] \u001b[2m2022-12-16T01:42:24.514871Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mEvaluation complete           \u001b[0m \u001b[36mtest_loss\u001b[0m=\u001b[35m0.02445993572473526\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T01:42:24.804693Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m1000\u001b[0m\n",
      "\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:02<02:31,  2.45s/it]\u001b[A\n",
      "  3%|▎         | 2/63 [00:04<02:11,  2.16s/it]\u001b[A\n",
      "  5%|▍         | 3/63 [00:06<02:03,  2.07s/it]\u001b[A\n",
      "  6%|▋         | 4/63 [00:08<01:59,  2.02s/it]\u001b[A\n",
      "  8%|▊         | 5/63 [00:10<01:55,  2.00s/it]\u001b[A\n",
      " 10%|▉         | 6/63 [00:12<01:52,  1.98s/it]\u001b[A\n",
      " 11%|█         | 7/63 [00:14<01:50,  1.97s/it]\u001b[A\n",
      " 13%|█▎        | 8/63 [00:16<01:48,  1.97s/it]\u001b[A\n",
      " 14%|█▍        | 9/63 [00:18<01:45,  1.96s/it]\u001b[A\n",
      " 16%|█▌        | 10/63 [00:20<01:43,  1.96s/it]\u001b[A\n",
      " 17%|█▋        | 11/63 [00:21<01:41,  1.96s/it]\u001b[A\n",
      " 19%|█▉        | 12/63 [00:23<01:39,  1.96s/it]\u001b[A\n",
      " 21%|██        | 13/63 [00:25<01:37,  1.96s/it]\u001b[A\n",
      " 22%|██▏       | 14/63 [00:27<01:35,  1.95s/it]\u001b[A\n",
      " 24%|██▍       | 15/63 [00:29<01:33,  1.95s/it]\u001b[A\n",
      " 25%|██▌       | 16/63 [00:31<01:31,  1.95s/it]\u001b[A\n",
      " 27%|██▋       | 17/63 [00:33<01:29,  1.95s/it]\u001b[A\n",
      " 29%|██▊       | 18/63 [00:35<01:27,  1.95s/it]\u001b[A\n",
      " 30%|███       | 19/63 [00:37<01:25,  1.95s/it]\u001b[A\n",
      " 32%|███▏      | 20/63 [00:39<01:23,  1.95s/it]\u001b[A\n",
      " 33%|███▎      | 21/63 [00:41<01:21,  1.95s/it]\u001b[A\n",
      " 35%|███▍      | 22/63 [00:43<01:19,  1.95s/it]\u001b[A\n",
      " 37%|███▋      | 23/63 [00:45<01:17,  1.95s/it]\u001b[A\n",
      " 38%|███▊      | 24/63 [00:47<01:15,  1.95s/it]\u001b[A\n",
      " 40%|███▉      | 25/63 [00:49<01:13,  1.95s/it]\u001b[A\n",
      " 41%|████▏     | 26/63 [00:51<01:12,  1.95s/it]\u001b[A\n",
      " 43%|████▎     | 27/63 [00:53<01:10,  1.95s/it]\u001b[A\n",
      " 44%|████▍     | 28/63 [00:55<01:08,  1.95s/it]\u001b[A\n",
      " 46%|████▌     | 29/63 [00:57<01:06,  1.95s/it]\u001b[A\n",
      " 48%|████▊     | 30/63 [00:59<01:04,  1.95s/it]\u001b[A\n",
      " 49%|████▉     | 31/63 [01:00<01:02,  1.95s/it]\u001b[A\n",
      " 51%|█████     | 32/63 [01:02<01:00,  1.95s/it]\u001b[A\n",
      " 52%|█████▏    | 33/63 [01:04<00:58,  1.95s/it]\u001b[A\n",
      " 54%|█████▍    | 34/63 [01:06<00:56,  1.95s/it]\u001b[A\n",
      " 56%|█████▌    | 35/63 [01:08<00:54,  1.95s/it]\u001b[A\n",
      " 57%|█████▋    | 36/63 [01:10<00:53,  1.97s/it]\u001b[A\n",
      " 59%|█████▊    | 37/63 [01:12<00:51,  1.96s/it]\u001b[A\n",
      " 60%|██████    | 38/63 [01:14<00:48,  1.96s/it]\u001b[A\n",
      " 62%|██████▏   | 39/63 [01:16<00:47,  1.96s/it]\u001b[A\n",
      " 63%|██████▎   | 40/63 [01:18<00:45,  1.96s/it]\u001b[A\n",
      " 65%|██████▌   | 41/63 [01:20<00:43,  1.96s/it]\u001b[A\n",
      " 67%|██████▋   | 42/63 [01:22<00:41,  1.95s/it]\u001b[A\n",
      " 68%|██████▊   | 43/63 [01:24<00:39,  1.95s/it]\u001b[A\n",
      " 70%|██████▉   | 44/63 [01:26<00:37,  1.95s/it]\u001b[A\n",
      " 71%|███████▏  | 45/63 [01:28<00:35,  1.95s/it]\u001b[A\n",
      " 73%|███████▎  | 46/63 [01:30<00:33,  1.95s/it]\u001b[A\n",
      " 75%|███████▍  | 47/63 [01:32<00:31,  1.95s/it]\u001b[A\n",
      " 76%|███████▌  | 48/63 [01:34<00:29,  1.95s/it]\u001b[A\n",
      " 78%|███████▊  | 49/63 [01:36<00:27,  1.95s/it]\u001b[A\n",
      " 79%|███████▉  | 50/63 [01:38<00:25,  1.95s/it]\u001b[A\n",
      " 81%|████████  | 51/63 [01:40<00:23,  1.95s/it]\u001b[A\n",
      " 83%|████████▎ | 52/63 [01:41<00:21,  1.95s/it]\u001b[A\n",
      " 84%|████████▍ | 53/63 [01:43<00:19,  1.95s/it]\u001b[A\n",
      " 86%|████████▌ | 54/63 [01:45<00:17,  1.95s/it]\u001b[A\n",
      " 87%|████████▋ | 55/63 [01:47<00:15,  1.95s/it]\u001b[A\n",
      " 89%|████████▉ | 56/63 [01:49<00:13,  1.95s/it]\u001b[A\n",
      " 90%|█████████ | 57/63 [01:51<00:11,  1.95s/it]\u001b[A\n",
      " 92%|█████████▏| 58/63 [01:53<00:09,  1.95s/it]\u001b[A\n",
      " 94%|█████████▎| 59/63 [01:55<00:07,  1.95s/it]\u001b[A\n",
      " 95%|█████████▌| 60/63 [01:57<00:05,  1.95s/it]\u001b[A\n",
      " 97%|█████████▋| 61/63 [01:59<00:03,  1.95s/it]\u001b[A\n",
      " 98%|█████████▊| 62/63 [02:01<00:01,  1.95s/it]\u001b[A\n",
      "100%|██████████| 63/63 [02:02<00:00,  1.95s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 1/3 [06:53<13:46, 413.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_size': 120,\n",
      " 'test_loss': 0.02445993572473526,\n",
      " 'train_loss': 0.027370166033506393}\n",
      "[802455-MainThread] [baal.modelwrapper:train_on_dataset:83] \u001b[2m2022-12-16T01:44:27.516073Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStarting training             \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m180\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m30\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:train_on_dataset:94] \u001b[2m2022-12-16T01:50:00.997024Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mTraining complete             \u001b[0m \u001b[36mtrain_loss\u001b[0m=\u001b[35m0.02446247637271881\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:test_on_dataset:123] \u001b[2m2022-12-16T01:50:01.004492Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStarting evaluating           \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m990\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:test_on_dataset:133] \u001b[2m2022-12-16T01:50:47.317267Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mEvaluation complete           \u001b[0m \u001b[36mtest_loss\u001b[0m=\u001b[35m0.024349955841898918\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T01:50:47.719892Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m1000\u001b[0m\n",
      "\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:02<02:37,  2.54s/it]\u001b[A\n",
      "  3%|▎         | 2/63 [00:04<02:13,  2.19s/it]\u001b[A\n",
      "  5%|▍         | 3/63 [00:06<02:04,  2.08s/it]\u001b[A\n",
      "  6%|▋         | 4/63 [00:08<01:59,  2.03s/it]\u001b[A\n",
      "  8%|▊         | 5/63 [00:10<01:55,  2.00s/it]\u001b[A\n",
      " 10%|▉         | 6/63 [00:12<01:53,  1.98s/it]\u001b[A\n",
      " 11%|█         | 7/63 [00:14<01:50,  1.97s/it]\u001b[A\n",
      " 13%|█▎        | 8/63 [00:16<01:48,  1.97s/it]\u001b[A\n",
      " 14%|█▍        | 9/63 [00:18<01:45,  1.96s/it]\u001b[A\n",
      " 16%|█▌        | 10/63 [00:20<01:43,  1.95s/it]\u001b[A\n",
      " 17%|█▋        | 11/63 [00:22<01:41,  1.95s/it]\u001b[A\n",
      " 19%|█▉        | 12/63 [00:23<01:39,  1.96s/it]\u001b[A\n",
      " 21%|██        | 13/63 [00:25<01:37,  1.95s/it]\u001b[A\n",
      " 22%|██▏       | 14/63 [00:27<01:35,  1.96s/it]\u001b[A\n",
      " 24%|██▍       | 15/63 [00:29<01:33,  1.95s/it]\u001b[A\n",
      " 25%|██▌       | 16/63 [00:31<01:31,  1.95s/it]\u001b[A\n",
      " 27%|██▋       | 17/63 [00:33<01:29,  1.95s/it]\u001b[A\n",
      " 29%|██▊       | 18/63 [00:35<01:27,  1.95s/it]\u001b[A\n",
      " 30%|███       | 19/63 [00:37<01:25,  1.95s/it]\u001b[A\n",
      " 32%|███▏      | 20/63 [00:39<01:23,  1.95s/it]\u001b[A\n",
      " 33%|███▎      | 21/63 [00:41<01:21,  1.95s/it]\u001b[A\n",
      " 35%|███▍      | 22/63 [00:43<01:20,  1.95s/it]\u001b[A\n",
      " 37%|███▋      | 23/63 [00:45<01:18,  1.95s/it]\u001b[A\n",
      " 38%|███▊      | 24/63 [00:47<01:16,  1.95s/it]\u001b[A\n",
      " 40%|███▉      | 25/63 [00:49<01:14,  1.95s/it]\u001b[A\n",
      " 41%|████▏     | 26/63 [00:51<01:12,  1.95s/it]\u001b[A\n",
      " 43%|████▎     | 27/63 [00:53<01:10,  1.95s/it]\u001b[A\n",
      " 44%|████▍     | 28/63 [00:55<01:08,  1.95s/it]\u001b[A\n",
      " 46%|████▌     | 29/63 [00:57<01:06,  1.95s/it]\u001b[A\n",
      " 48%|████▊     | 30/63 [00:59<01:04,  1.95s/it]\u001b[A\n",
      " 49%|████▉     | 31/63 [01:01<01:02,  1.95s/it]\u001b[A\n",
      " 51%|█████     | 32/63 [01:03<01:00,  1.95s/it]\u001b[A\n",
      " 52%|█████▏    | 33/63 [01:04<00:58,  1.95s/it]\u001b[A\n",
      " 54%|█████▍    | 34/63 [01:06<00:56,  1.95s/it]\u001b[A\n",
      " 56%|█████▌    | 35/63 [01:08<00:54,  1.95s/it]\u001b[A\n",
      " 57%|█████▋    | 36/63 [01:10<00:52,  1.95s/it]\u001b[A\n",
      " 59%|█████▊    | 37/63 [01:12<00:50,  1.95s/it]\u001b[A\n",
      " 60%|██████    | 38/63 [01:14<00:48,  1.95s/it]\u001b[A\n",
      " 62%|██████▏   | 39/63 [01:16<00:46,  1.95s/it]\u001b[A\n",
      " 63%|██████▎   | 40/63 [01:18<00:44,  1.95s/it]\u001b[A\n",
      " 65%|██████▌   | 41/63 [01:20<00:42,  1.95s/it]\u001b[A\n",
      " 67%|██████▋   | 42/63 [01:22<00:40,  1.95s/it]\u001b[A\n",
      " 68%|██████▊   | 43/63 [01:24<00:39,  1.95s/it]\u001b[A\n",
      " 70%|██████▉   | 44/63 [01:26<00:37,  1.95s/it]\u001b[A\n",
      " 71%|███████▏  | 45/63 [01:28<00:35,  1.95s/it]\u001b[A\n",
      " 73%|███████▎  | 46/63 [01:30<00:33,  1.95s/it]\u001b[A\n",
      " 75%|███████▍  | 47/63 [01:32<00:31,  1.95s/it]\u001b[A\n",
      " 76%|███████▌  | 48/63 [01:34<00:29,  1.95s/it]\u001b[A\n",
      " 78%|███████▊  | 49/63 [01:36<00:27,  1.95s/it]\u001b[A\n",
      " 79%|███████▉  | 50/63 [01:38<00:25,  1.95s/it]\u001b[A\n",
      " 81%|████████  | 51/63 [01:40<00:23,  1.95s/it]\u001b[A\n",
      " 83%|████████▎ | 52/63 [01:42<00:21,  1.95s/it]\u001b[A\n",
      " 84%|████████▍ | 53/63 [01:44<00:19,  1.95s/it]\u001b[A\n",
      " 86%|████████▌ | 54/63 [01:45<00:17,  1.95s/it]\u001b[A\n",
      " 87%|████████▋ | 55/63 [01:47<00:15,  1.95s/it]\u001b[A\n",
      " 89%|████████▉ | 56/63 [01:49<00:13,  1.95s/it]\u001b[A\n",
      " 90%|█████████ | 57/63 [01:51<00:11,  1.95s/it]\u001b[A\n",
      " 92%|█████████▏| 58/63 [01:53<00:09,  1.95s/it]\u001b[A\n",
      " 94%|█████████▎| 59/63 [01:55<00:07,  1.95s/it]\u001b[A\n",
      " 95%|█████████▌| 60/63 [01:57<00:05,  1.95s/it]\u001b[A\n",
      " 97%|█████████▋| 61/63 [01:59<00:03,  1.95s/it]\u001b[A\n",
      " 98%|█████████▊| 62/63 [02:01<00:01,  1.95s/it]\u001b[A\n",
      "100%|██████████| 63/63 [02:02<00:00,  1.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 2/3 [15:16<07:46, 466.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'dataset_size': 180,\n",
      " 'test_loss': 0.024349955841898918,\n",
      " 'train_loss': 0.02446247637271881}\n",
      "[802455-MainThread] [baal.modelwrapper:train_on_dataset:83] \u001b[2m2022-12-16T01:52:50.710970Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStarting training             \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m240\u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m30\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:train_on_dataset:94] \u001b[2m2022-12-16T01:59:28.223262Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mTraining complete             \u001b[0m \u001b[36mtrain_loss\u001b[0m=\u001b[35m0.02128368429839611\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:test_on_dataset:123] \u001b[2m2022-12-16T01:59:28.242017Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStarting evaluating           \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m990\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:test_on_dataset:133] \u001b[2m2022-12-16T02:00:15.817962Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mEvaluation complete           \u001b[0m \u001b[36mtest_loss\u001b[0m=\u001b[35m0.024393050000071526\u001b[0m\n",
      "[802455-MainThread] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-16T02:00:16.114333Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m1000\u001b[0m\n",
      "\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/63 [00:02<02:32,  2.45s/it]\u001b[A\n",
      "  3%|▎         | 2/63 [00:04<02:12,  2.16s/it]\u001b[A\n",
      "  5%|▍         | 3/63 [00:06<02:04,  2.07s/it]\u001b[A\n",
      "  6%|▋         | 4/63 [00:08<01:59,  2.02s/it]\u001b[A\n",
      "  8%|▊         | 5/63 [00:10<01:56,  2.00s/it]\u001b[A\n",
      " 10%|▉         | 6/63 [00:12<01:53,  1.98s/it]\u001b[A\n",
      " 11%|█         | 7/63 [00:14<01:50,  1.98s/it]\u001b[A\n",
      " 13%|█▎        | 8/63 [00:16<01:48,  1.97s/it]\u001b[A\n",
      " 14%|█▍        | 9/63 [00:18<01:46,  1.96s/it]\u001b[A\n",
      " 16%|█▌        | 10/63 [00:20<01:44,  1.97s/it]\u001b[A\n",
      " 17%|█▋        | 11/63 [00:22<01:42,  1.97s/it]\u001b[A\n",
      " 19%|█▉        | 12/63 [00:24<01:40,  1.97s/it]\u001b[A\n",
      " 21%|██        | 13/63 [00:25<01:38,  1.97s/it]\u001b[A\n",
      " 22%|██▏       | 14/63 [00:27<01:36,  1.96s/it]\u001b[A\n",
      " 24%|██▍       | 15/63 [00:29<01:34,  1.96s/it]\u001b[A\n",
      " 25%|██▌       | 16/63 [00:31<01:32,  1.96s/it]\u001b[A\n",
      " 27%|██▋       | 17/63 [00:33<01:30,  1.96s/it]\u001b[A\n",
      " 29%|██▊       | 18/63 [00:35<01:28,  1.96s/it]\u001b[A\n",
      " 30%|███       | 19/63 [00:37<01:25,  1.95s/it]\u001b[A\n",
      " 32%|███▏      | 20/63 [00:39<01:24,  1.95s/it]\u001b[A\n",
      " 33%|███▎      | 21/63 [00:41<01:22,  1.96s/it]\u001b[A\n",
      " 35%|███▍      | 22/63 [00:43<01:20,  1.95s/it]\u001b[A\n",
      " 37%|███▋      | 23/63 [00:45<01:18,  1.96s/it]\u001b[A\n",
      " 38%|███▊      | 24/63 [00:47<01:16,  1.95s/it]\u001b[A\n",
      " 40%|███▉      | 25/63 [00:49<01:14,  1.96s/it]\u001b[A\n",
      " 41%|████▏     | 26/63 [00:51<01:12,  1.95s/it]\u001b[A\n",
      " 43%|████▎     | 27/63 [00:53<01:10,  1.95s/it]\u001b[A\n",
      " 44%|████▍     | 28/63 [00:55<01:08,  1.95s/it]\u001b[A\n",
      " 46%|████▌     | 29/63 [00:57<01:06,  1.96s/it]\u001b[A\n",
      " 48%|████▊     | 30/63 [00:59<01:04,  1.96s/it]\u001b[A\n",
      " 49%|████▉     | 31/63 [01:01<01:02,  1.96s/it]\u001b[A\n",
      " 51%|█████     | 32/63 [01:03<01:00,  1.95s/it]\u001b[A\n",
      " 52%|█████▏    | 33/63 [01:05<00:58,  1.95s/it]\u001b[A\n",
      " 54%|█████▍    | 34/63 [01:07<00:56,  1.95s/it]\u001b[A\n",
      " 56%|█████▌    | 35/63 [01:08<00:54,  1.95s/it]\u001b[A\n",
      " 57%|█████▋    | 36/63 [01:10<00:52,  1.95s/it]\u001b[A\n",
      " 59%|█████▊    | 37/63 [01:12<00:50,  1.95s/it]\u001b[A\n",
      " 60%|██████    | 38/63 [01:14<00:48,  1.96s/it]\u001b[A\n",
      " 62%|██████▏   | 39/63 [01:16<00:46,  1.96s/it]\u001b[A\n",
      " 63%|██████▎   | 40/63 [01:18<00:44,  1.96s/it]\u001b[A\n",
      " 65%|██████▌   | 41/63 [01:20<00:42,  1.95s/it]\u001b[A\n",
      " 67%|██████▋   | 42/63 [01:22<00:41,  1.96s/it]\u001b[A\n",
      " 68%|██████▊   | 43/63 [01:24<00:39,  1.95s/it]\u001b[A\n",
      " 70%|██████▉   | 44/63 [01:26<00:37,  1.95s/it]\u001b[A\n",
      " 71%|███████▏  | 45/63 [01:28<00:35,  1.95s/it]\u001b[A\n",
      " 73%|███████▎  | 46/63 [01:30<00:33,  1.95s/it]\u001b[A\n",
      " 75%|███████▍  | 47/63 [01:32<00:31,  1.96s/it]\u001b[A\n",
      " 76%|███████▌  | 48/63 [01:34<00:29,  1.95s/it]\u001b[A\n",
      " 78%|███████▊  | 49/63 [01:36<00:27,  1.95s/it]\u001b[A\n",
      " 79%|███████▉  | 50/63 [01:38<00:25,  1.95s/it]\u001b[A\n",
      " 81%|████████  | 51/63 [01:40<00:23,  1.95s/it]\u001b[A\n",
      " 83%|████████▎ | 52/63 [01:42<00:21,  1.95s/it]\u001b[A\n",
      " 84%|████████▍ | 53/63 [01:44<00:19,  1.95s/it]\u001b[A\n",
      " 86%|████████▌ | 54/63 [01:46<00:17,  1.95s/it]\u001b[A\n",
      " 87%|████████▋ | 55/63 [01:48<00:15,  1.95s/it]\u001b[A\n",
      " 89%|████████▉ | 56/63 [01:50<00:13,  1.96s/it]\u001b[A\n",
      " 90%|█████████ | 57/63 [01:51<00:11,  1.96s/it]\u001b[A\n",
      " 92%|█████████▏| 58/63 [01:53<00:09,  1.96s/it]\u001b[A\n",
      " 94%|█████████▎| 59/63 [01:55<00:07,  1.95s/it]\u001b[A\n",
      " 95%|█████████▌| 60/63 [01:57<00:05,  1.95s/it]\u001b[A\n",
      " 97%|█████████▋| 61/63 [01:59<00:03,  1.95s/it]\u001b[A\n",
      " 98%|█████████▊| 62/63 [02:01<00:01,  1.95s/it]\u001b[A\n",
      "100%|██████████| 63/63 [02:02<00:00,  1.95s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [24:45<00:00, 495.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_size': 240,\n",
      " 'test_loss': 0.024393050000071526,\n",
      " 'train_loss': 0.02128368429839611}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "heuristic = get_heuristic(p_heuristic, reduction=mean_regions)\n",
    "\n",
    "# The ALLoop is in charge of predicting the uncertainty and\n",
    "loop = ActiveLearningLoop(\n",
    "    active_set,\n",
    "    model.predict_on_dataset_generator,\n",
    "    heuristic=heuristic,\n",
    "    query_size=p_query_size,\n",
    "    # Instead of predicting on the entire pool, only a subset is used\n",
    "    max_sample=1000,\n",
    "    batch_size=p_batch_size,\n",
    "    iterations=p_query_interations,\n",
    "    use_cuda=use_cuda,\n",
    ")\n",
    "acc = []\n",
    "for epoch in tqdm(range(p_active_learning_steps)):\n",
    "    # Following Gal et al. 2016, we reset the weights.\n",
    "    model.load_state_dict(initial_weights)\n",
    "    # Train 50 epochs before sampling.\n",
    "    \n",
    "    model.train_on_dataset(\n",
    "        active_set, \n",
    "        optimizer, \n",
    "        p_batch_size,\n",
    "        p_leaning_epochs,\n",
    "        use_cuda,\n",
    "        workers=12\n",
    "    )\n",
    "\n",
    "    # Validation!\n",
    "    model.test_on_dataset(test_set, p_batch_size, use_cuda)\n",
    "    should_continue = loop.step()\n",
    "\n",
    "    logs = model.get_metrics()\n",
    "    pprint(logs)\n",
    "    acc.append(logs)\n",
    "    if not should_continue:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.02445993572473526,\n",
       "  'train_loss': 0.027370166033506393,\n",
       "  'dataset_size': 120},\n",
       " {'test_loss': 0.024349955841898918,\n",
       "  'train_loss': 0.02446247637271881,\n",
       "  'dataset_size': 180},\n",
       " {'test_loss': 0.024393050000071526,\n",
       "  'train_loss': 0.02128368429839611,\n",
       "  'dataset_size': 240}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
