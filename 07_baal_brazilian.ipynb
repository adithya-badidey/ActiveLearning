{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JYNEvsZBBxuO"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install baal\n",
        "# ! pip install segmentation_models_pytorch"
      ],
      "metadata": {
        "id": "pmsTrR0HCFog",
        "outputId": "e03f9952-d5f8-4a33-b417-38546fb4766b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting segmentation_models_pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.1-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from segmentation_models_pytorch) (0.14.0+cu116)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from segmentation_models_pytorch) (4.64.1)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 31.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from segmentation_models_pytorch) (7.1.2)\n",
            "Collecting efficientnet-pytorch==0.7.1\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.13.0+cu116)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (1.24.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=886c7ba658bb0a50da4c6e958acf4b5bd2745c5effde76fdd227672ee2d81dc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/b9/90/25a0195cf95fb5533db96f1c77ea3f296b7cc86ae8ae48e3dc\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60966 sha256=9d7c6171681cd085bd2496261af7d0bb46426537093d618f28b136b1a51249fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/fa/b9/5c82b59d905f95542a192b883c0cc0082407ea2f54beb2f9e6\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.1 timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KM6I2WSiBmMu"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "import baal\n",
        "from baal import ActiveLearningDataset\n",
        "from baal.active import get_heuristic, ActiveLearningLoop\n",
        "from baal.bayesian.dropout import MCDropoutModule\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from PIL import Image\n",
        "from pprint import pprint\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from torchvision.transforms import transforms\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from typing import List\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yI5yBW14BxuQ",
        "outputId": "f74dc3ce-09c8-4416-b9f3-7b611265a6f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch: \t\t 1.13.0+cu116\n",
            "GPU:         \t\t A100-SXM4-40GB\n",
            "Memory Usage:\t 0.0 GB /  0.0 GB\n"
          ]
        }
      ],
      "source": [
        "debug=False\n",
        "\n",
        "print(\"Pytorch: \\t\\t\", torch.__version__)\n",
        "if not debug and torch.cuda.is_available():\n",
        "    device='cuda'\n",
        "    print('GPU:         \\t\\t', torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:\\t',\n",
        "        round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB / ',\n",
        "        round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
        "    \n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    print(\"GPU is **not available**\")\n",
        "    device='cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OMYkCTgRWBxe",
        "outputId": "b93c0c0d-bfff-4923-e73f-b7a040c21217",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Updated Python Path\n",
            "Project Root Path: /content\n",
            "Project Source Root Path: /content\n",
            "Project Data Path: /content/data\n"
          ]
        }
      ],
      "source": [
        "# Initialization\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import sys\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "# To be able to reference packages/modules in this repository, this\n",
        "# relative path must be added to the python path. Your notebook may be \n",
        "# in a different folder, so modify this variable to point to the src \n",
        "# folder.\n",
        "proj_notebooks_root = pathlib.Path().absolute()\n",
        "proj_root_path = proj_notebooks_root\n",
        "data_path = proj_notebooks_root / \"data\"\n",
        "\n",
        "if proj_root_path not in sys.path:\n",
        "    sys.path.insert(0, proj_root_path.as_posix())\n",
        "    print(\"Updated Python Path\")\n",
        "\n",
        "print(f\"Project Root Path: {proj_root_path}\")\n",
        "print(f\"Project Source Root Path: {proj_notebooks_root}\")\n",
        "print(f\"Project Data Path: {data_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CJ1_JkzMBxuR",
        "outputId": "a4024f46-a8fb-4cc7-9b96-ac04c30530a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-17 03:03:44--  https://data.lib.vt.edu/ndownloader/articles/16625056/versions/1\n",
            "Resolving data.lib.vt.edu (data.lib.vt.edu)... 52.51.172.108, 63.32.251.213, 2a05:d018:1f4:d000:3522:f0cc:fbbc:af1f, ...\n",
            "Connecting to data.lib.vt.edu (data.lib.vt.edu)|52.51.172.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1028474638 (981M) [application/zip]\n",
            "Saving to: ‘/content/data/1’\n",
            "\n",
            "1                   100%[===================>] 980.83M  30.6MB/s    in 33s     \n",
            "\n",
            "2022-12-17 03:04:17 (29.9 MB/s) - ‘/content/data/1’ saved [1028474638/1028474638]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "base_dir = data_path / 'ConglomerateConcreteCrackDataset'\n",
        "if not base_dir.exists():\n",
        "    dataset_url = 'https://data.lib.vt.edu/ndownloader/articles/16625056/versions/1'\n",
        "\n",
        "    ! wget {dataset_url} -P {data_path}\n",
        "    ! unzip -q {data_path / '1'} -d {data_path}\n",
        "    ! unzip -q {data_path / 'Conglomerate\\ Concrete\\ Crack\\ Detection.zip'} -d {data_path}\n",
        "    ! mv {data_path/'Conglomerate\\ Concrete\\ Crack\\ Detection'} {base_dir}\n",
        "    ! mv {data_path / 'README_congl_dataset.rtf'}  {data_path/'ConglomerateConcreteCrackDataset'}     \n",
        "    ! rm {data_path / 'Conglomerate\\ Concrete\\ Crack\\ Detection.zip'}\n",
        "    ! rm {data_path / '1'}\n",
        "else:\n",
        "    print(\"Found dataset at \", base_dir.as_posix())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wEvOwdO3CQrM"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    References:\n",
        "        Author: clcarwin\n",
        "        Site https://github.com/clcarwin/focal_loss_pytorch/blob/master/focalloss.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        if isinstance(alpha, (float, int)):\n",
        "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
        "        if isinstance(alpha, list):\n",
        "            self.alpha = torch.Tensor(alpha)\n",
        "        self.size_average = size_average\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if input.dim() > 2:\n",
        "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
        "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
        "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
        "        target = target.view(-1, 1)\n",
        "\n",
        "        logpt = F.log_softmax(input, dim=1)\n",
        "        logpt = logpt.gather(1, target)\n",
        "        logpt = logpt.view(-1)\n",
        "        pt = logpt.data.exp()\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            if self.alpha.type() != input.data.type():\n",
        "                self.alpha = self.alpha.type_as(input.data)\n",
        "            select = (target != 0).type(torch.LongTensor).to(self.alpha.device)\n",
        "            at = self.alpha.gather(0, select.data.view(-1))\n",
        "            logpt = logpt * at\n",
        "\n",
        "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
        "        if self.size_average:\n",
        "            return loss.mean()\n",
        "        else:\n",
        "            return loss.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XMuIz-pmFDU3"
      },
      "outputs": [],
      "source": [
        "def mean_regions(n, grid_size=16):\n",
        "    # Compute the mean uncertainty per regions.\n",
        "    # [batch_size, W, H]\n",
        "    n = torch.from_numpy(n[:, None, ...])\n",
        "    # [Batch_size, 1, grid, grid]\n",
        "    out = F.adaptive_avg_pool2d(n, grid_size)\n",
        "    return np.mean(out.view([-1, grid_size**2]).numpy(), -1)\n",
        "\n",
        "\n",
        "class ArrayDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, array, image_transforms=None, both_transforms=None):\n",
        "        self.array = array\n",
        "                \n",
        "        self.image_transforms = image_transforms\n",
        "        self.segment_transforms = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.array)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        imagePath, segmentPath = self.array[index]\n",
        "        image = Image.open(imagePath).convert('RGB')\n",
        "        segment = Image.open(segmentPath).convert('L')\n",
        "        segment = self.segment_transforms(segment)        \n",
        "            \n",
        "        if self.image_transforms is not None:\n",
        "            image = self.image_transforms(image)\n",
        "        \n",
        "        # print(image.shape, segment.shape)\n",
        "\n",
        "        if image.shape != (3, 448, 448):\n",
        "            print(f\"Image shape is {image.shape}\")\n",
        "        if segment.shape != (1, 448, 448):\n",
        "            print(f\"Segment shape is {segment.shape}\")\n",
        "        \n",
        "        return image, segment.type(torch.float32) \n",
        "    \n",
        "    def split(self, p=0.5):\n",
        "        count = len(self.array)\n",
        "        index = np.arange(count)\n",
        "        first = int(count * p)\n",
        "        return [\n",
        "            ArrayDataset(self.array[index[:first]], \n",
        "                    image_transforms=self.image_transforms), \n",
        "            ArrayDataset(self.array[index[first:]], \n",
        "                    image_transforms=self.image_transforms)\n",
        "        ]\n",
        "    \n",
        "    def split_count(self, first):\n",
        "        return [\n",
        "            ArrayDataset(self.array[:first], \n",
        "                    image_transforms=self.image_transforms), \n",
        "            ArrayDataset(self.array[first:], \n",
        "                    image_transforms=self.image_transforms)\n",
        "        ]\n",
        "\n",
        "def get_datasets(initial_pool, path):\n",
        "    transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    X_dir = path/'Train'/'images'\n",
        "    y_dir = path/'Train'/'masks'\n",
        "\n",
        "    files = [y for y in X_dir.glob('*')][:5000]\n",
        "\n",
        "    for i in files:\n",
        "        assert((y_dir / i.name).exists())\n",
        "\n",
        "    data = np.array([(i, (y_dir / i.name)) for id, i in enumerate(files)])\n",
        "\n",
        "    dataset = ArrayDataset(data, image_transforms = transform)\n",
        "\n",
        "\n",
        "    test_set, active_set = dataset.split_count(300)\n",
        "    print(\"Active Set: \", len(active_set))\n",
        "    print(\"Test Set: \", len(test_set))\n",
        "    \n",
        "    active_set = ActiveLearningDataset(active_set)\n",
        "    \n",
        "    active_set.label_randomly(initial_pool)\n",
        "    return active_set, test_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xUATwMfVBxuS",
        "outputId": "1b2b74cd-5ea3-461d-e366-3df4bd16ab8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use Cuda: True\n"
          ]
        }
      ],
      "source": [
        "p_active_learning_steps = 4\n",
        "\n",
        "p_initial_pool = 300\n",
        "p_query_size = 300\n",
        "p_query_interations = 20\n",
        "\n",
        "p_heuristic = \"random\"\n",
        "p_reduce=\"sum\"\n",
        "\n",
        "p_learning_epochs=20\n",
        "p_batch_size = 64\n",
        "p_learning_rate = 0.001\n",
        "\n",
        "p_classes = 1\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "# use_cuda = False\n",
        "print(\"Use Cuda:\", use_cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ffgRSSekBxuU"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_eval_metrics(pred, gold, threshold = 0.5):\n",
        "    pred = (pred > threshold).long()\n",
        "    gold = gold.long()\n",
        "    hits = torch.sum(torch.mul(pred, gold)).item() #element-wise multiplication\n",
        "    shots = torch.sum(pred).item()\n",
        "    targets = torch.sum(gold).item()\n",
        "#     print(hits, shots, targets)\n",
        "    return hits, shots, targets\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer, scaler, history=None, lr_sched=None):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    loss_sum = 0\n",
        "    total_hits = 0\n",
        "    total_shots = 0\n",
        "    total_targets = 0\n",
        "    for X, y in dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Runs the forward pass with autocasting.\n",
        "        with autocast():\n",
        "            output = model(X)\n",
        "            loss = loss_fn(output, y)\n",
        "\n",
        "            hits, shots, targets = get_eval_metrics(output, y)\n",
        "            total_hits += hits\n",
        "            total_shots += shots\n",
        "            total_targets += targets\n",
        "\n",
        "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "        # Backward passes under autocast are not recommended.\n",
        "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
        "        # otherwise, optimizer.step() is skipped.\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        # Updates the scale for next iteration.\n",
        "        scaler.update()\n",
        "\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        if(lr_sched is not None):\n",
        "            lr_sched.step()\n",
        "\n",
        "    history['train_loss'].append(loss_sum/len(dataloader))\n",
        "    history['train_hits'].append(total_hits)\n",
        "    history['train_shots'].append(total_shots)\n",
        "    history['train_targets'].append(total_targets)\n",
        "    \n",
        "    if total_hits == 0 or total_shots == 0:\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1score = 0\n",
        "    else:\n",
        "        precision = total_hits/total_shots\n",
        "        recall = total_hits/total_targets\n",
        "        f1score = (2 * precision * recall)/(precision + recall)\n",
        "        \n",
        "\n",
        "    history['train_precision'].append(precision)\n",
        "    history['train_recall'].append(recall)\n",
        "    history['train_f1score'].append(f1score)\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, history):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    loss = 0\n",
        "    total_hits = 0\n",
        "    total_shots = 0\n",
        "    total_targets = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "#             print(y.shape)\n",
        "            \n",
        "            output = model(X)\n",
        "            \n",
        "            loss += loss_fn(output, y).item()\n",
        "            hits, shots, targets = get_eval_metrics(output, y)\n",
        "            total_hits += hits\n",
        "            total_shots += shots\n",
        "            total_targets += targets\n",
        "\n",
        "    loss /= num_batches\n",
        "    history['test_loss'].append(loss)\n",
        "    history['test_hits'].append(total_hits)\n",
        "    history['test_shots'].append(total_shots)\n",
        "    history['test_targets'].append(total_targets)\n",
        "    \n",
        "    if total_hits == 0 or total_shots == 0:\n",
        "        precision = 0\n",
        "        recall = 0\n",
        "        f1score = 0\n",
        "    else:\n",
        "        precision = total_hits/total_shots\n",
        "        recall = total_hits/total_targets\n",
        "        f1score = (2 * precision * recall)/(precision + recall)\n",
        "        \n",
        "\n",
        "    history['test_precision'].append(precision)\n",
        "    history['test_recall'].append(recall)\n",
        "    history['test_f1score'].append(f1score)\n",
        "\n",
        "\n",
        "def train_model(model_wrapper, loss_fn, train_dataset, test_dataset,epochs=40):\n",
        "    history = {\n",
        "        'train_loss':[],\n",
        "        'train_hits':[],\n",
        "        'train_shots':[],\n",
        "        'train_targets':[],\n",
        "        'train_precision': [],\n",
        "        'train_recall': [],\n",
        "        'train_f1score': [],\n",
        "        'test_loss':[],\n",
        "        'test_hits':[],\n",
        "        'test_shots':[],\n",
        "        'test_targets':[],\n",
        "        'test_precision': [],\n",
        "        'test_recall': [],\n",
        "        'test_f1score': []\n",
        "    }\n",
        "\n",
        "    model_wrapper.model\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
        "                            batch_size=p_batch_size,\n",
        "                            shuffle=True,\n",
        "                            prefetch_factor=2,\n",
        "                            pin_memory=True,\n",
        "                            num_workers=12)\n",
        "\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
        "                            batch_size=p_batch_size,\n",
        "                            shuffle=False,\n",
        "                            num_workers=12)\n",
        "        \n",
        "    scaler = GradScaler()\n",
        "    params = [p for p in model_wrapper.model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.SGD(\n",
        "        params, \n",
        "        lr=p_learning_rate, \n",
        "        momentum=0.9, \n",
        "        weight_decay=5e-4)\n",
        "    \n",
        "    for t in tqdm(range(epochs), bar_format=\"{elapsed} Elapsed | {percentage:3.0f}% done |{bar}| {n_fmt}/{total_fmt} [{remaining} remaining | {rate_fmt}{postfix}]\", unit=\"epoch\", total=epochs):\n",
        "#         dataloader, model, loss_fn, optimizer, scaler, history=None\n",
        "        train_loop(train_dataloader, model_wrapper.model, loss_fn, optimizer, \n",
        "                   scaler, history, None)\n",
        "        test_loop(test_dataloader, model_wrapper.model, loss_fn, history)\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "UM7yVFTxBxuW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# We will use the FocalLoss\n",
        "if p_classes > 1:\n",
        "    criterion = FocalLoss(gamma=2, alpha=0.25)\n",
        "else:\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# \n",
        "# model = smp.Unet(\n",
        "#     encoder_name=\"resnext50_32x4d\",\n",
        "#     encoder_depth=5,\n",
        "#     encoder_weights=\"imagenet\",\n",
        "#     decoder_use_batchnorm=False,\n",
        "#     classes=1,\n",
        "# )\n",
        "# add_dropout(model, classes=1, activation=None)\n",
        "\n",
        "model = smp.Unet('resnet34', \n",
        "                 classes=p_classes,\n",
        "                 encoder_weights=\"imagenet\",\n",
        "                 decoder_use_batchnorm=False)\n",
        "model.segmentation_head[1] = nn.Dropout2d(0.5)\n",
        "\n",
        "\n",
        "# This will enable Dropout at test time.\n",
        "model = MCDropoutModule(model)\n",
        "\n",
        "# Put everything on GPU.\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "# Keep a copy of the original weights\n",
        "initial_weights = deepcopy(model.state_dict())\n",
        "\n",
        "# Add metrics\n",
        "model = baal.ModelWrapper(model, criterion, replicate_in_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "scrolled": true,
        "id": "AfTLOIhlBxuX",
        "outputId": "46600d04-0bfe-4ec9-fd2e-d805fb723437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================\n",
            "Heuristic: random\n",
            "====================\n",
            "Active Set:  4700\n",
            "Test Set:  300\n",
            "Step: 1\n",
            "Training Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "01:42 Elapsed | 100% done |██████████| 20/20 [00:00 remaining |  5.13s/epoch]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last Training:\n",
            "train_loss:0.414\n",
            "train_hits:58.000\n",
            "train_shots:2220.000\n",
            "train_targets:1164745.000\n",
            "train_precision:0.026\n",
            "train_recall:0.000\n",
            "train_f1score:0.000\n",
            "test_loss :0.398\n",
            "test_hits :59.000\n",
            "test_shots:2035.000\n",
            "test_targets:1240986.000\n",
            "test_precision:0.029\n",
            "test_recall:0.000\n",
            "test_f1score:0.000\n",
            "Querying...\n",
            "\n",
            "Querying...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _ConnectionBase.__del__ at 0x7ff4bfb975e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 132, in __del__\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/multiprocessing/queues.py\", line 235, in _feed\n",
            "    close()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 177, in close\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n",
            "    self._close()\n",
            "  File \"/usr/lib/python3.8/multiprocessing/connection.py\", line 361, in _close\n",
            "    _close(self._handle)\n",
            "OSError: [Errno 9] Bad file descriptor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[114-MainThread  ] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-17T03:11:54.677621Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m1000\u001b[0m\n",
            "100%|██████████| 16/16 [00:38<00:00,  2.38s/it]\n",
            "--------------------\n",
            "Step: 2\n",
            "Training Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "02:32 Elapsed | 100% done |██████████| 20/20 [00:00 remaining |  7.60s/epoch]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last Training:\n",
            "train_loss:0.381\n",
            "train_hits:70.000\n",
            "train_shots:2018.000\n",
            "train_targets:2112614.000\n",
            "train_precision:0.035\n",
            "train_recall:0.000\n",
            "train_f1score:0.000\n",
            "test_loss :0.377\n",
            "test_hits :42.000\n",
            "test_shots:879.000\n",
            "test_targets:1240986.000\n",
            "test_precision:0.048\n",
            "test_recall:0.000\n",
            "test_f1score:0.000\n",
            "Querying...\n",
            "\n",
            "Querying...\n",
            "[114-MainThread  ] [baal.modelwrapper:predict_on_dataset_generator:232] \u001b[2m2022-12-17T03:15:04.957295Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStart Predict                 \u001b[0m \u001b[36mdataset\u001b[0m=\u001b[35m1000\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 16/16 [00:38<00:00,  2.40s/it]\n",
            "--------------------\n",
            "Step: 3\n",
            "Training Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "02:23 Elapsed |  75% done |███████▌  | 15/20 [00:47 remaining |  9.55s/epoch]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-21135bb54f08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         model, stats = train_model(model, criterion, \n\u001b[0m\u001b[1;32m     39\u001b[0m                                    \u001b[0mactive_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                    epochs=p_learning_epochs)\n",
            "\u001b[0;32m<ipython-input-34-235ad704af9c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_wrapper, loss_fn, train_dataset, test_dataset, epochs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{elapsed} Elapsed | {percentage:3.0f}% done |{bar}| {n_fmt}/{total_fmt} [{remaining} remaining | {rate_fmt}{postfix}]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m#         dataloader, model, loss_fn, optimizer, scaler, history=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         train_loop(train_dataloader, model_wrapper.model, loss_fn, optimizer, \n\u001b[0m\u001b[1;32m    154\u001b[0m                    scaler, history, None)\n\u001b[1;32m    155\u001b[0m         \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-235ad704af9c>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, scaler, history, lr_sched)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# If these gradients do not contain infs or NaNs, optimizer.step() is then called,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# otherwise, optimizer.step() is skipped.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Updates the scale for next iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "heuristics = ['random', 'entropy', 'bald']\n",
        "all_histories = []\n",
        "for h in heuristics:\n",
        "    print(\"=\"*20)\n",
        "    print(f\"Heuristic: {h}\")\n",
        "    print(\"=\"*20)\n",
        "    if h == 'batch_bald':\n",
        "        heuristic = get_heuristic(h, reduction=mean_regions, num_samples=1000)\n",
        "    else:\n",
        "        heuristic = get_heuristic(h, reduction=mean_regions)\n",
        "\n",
        "    active_set, test_set = get_datasets(p_initial_pool, base_dir)\n",
        "\n",
        "    image, segment = active_set[0]\n",
        "    image.shape, segment.shape\n",
        "\n",
        "    # The ALLoop is in charge of predicting the uncertainty and\n",
        "    loop = ActiveLearningLoop(\n",
        "        active_set,\n",
        "        model.predict_on_dataset_generator,\n",
        "        heuristic=heuristic,\n",
        "        query_size=p_query_size,\n",
        "        # Instead of predicting on the entire pool, only a subset is used\n",
        "        max_sample=1000,\n",
        "        batch_size=p_batch_size,\n",
        "        iterations=p_query_interations,\n",
        "        use_cuda=use_cuda,\n",
        "    )\n",
        "    history = []\n",
        "    all_histories.append(history)\n",
        "    for epoch in range(p_active_learning_steps):\n",
        "        print(f\"Step: {epoch + 1}\")\n",
        "        # Following Gal et al. 2016, we reset the weights.\n",
        "        model.load_state_dict(initial_weights)\n",
        "        # Train 50 epochs before sampling.\n",
        "\n",
        "        print(\"Training Model...\")\n",
        "        model, stats = train_model(model, criterion, \n",
        "                                   active_set, test_set, \n",
        "                                   epochs=p_learning_epochs)\n",
        "        history.append(stats)\n",
        "        print(\"Last Training:\")\n",
        "        for i in stats:\n",
        "            print(f\"{i:10}:{stats[i][-1]:.3f}\")\n",
        "        print(\"Querying...\")\n",
        "        print()\n",
        "\n",
        "        print(\"Querying...\")\n",
        "\n",
        "        should_continue = loop.step()\n",
        "\n",
        "        print(\"-\"*20)\n",
        "\n",
        "        if not should_continue:\n",
        "            break\n",
        "    \n",
        "    print(\"Training Final Model...\")\n",
        "    model, stats = train_model(model, criterion, \n",
        "                                active_set, test_set, \n",
        "                                epochs=p_learning_epochs)\n",
        "    history.append(stats)\n",
        "    print(\"Last Training:\")\n",
        "    for i in stats:\n",
        "        print(f\"{i:10}:{stats[i][-1]:.3f}\")\n",
        "    print(\"Querying...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd0Hp2XMBxuX"
      },
      "outputs": [],
      "source": [
        "len(all_histories[-1]), all_histories[-1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EgUbJMbBxuX"
      },
      "outputs": [],
      "source": [
        "len(all_histories), len(all_histories[0]), all_histories[0][0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNLW_ZXvBxuX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('log.txt', 'w') as logfile:\n",
        "    logfile.write(json.dumps(all_histories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-cb6TkkBxuY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        " \n",
        "with open('data.json') as json_file:\n",
        "    data = json.load(json_file)\n",
        " \n",
        "    # Print the type of data variable\n",
        "    print(\"Type:\", type(data))\n",
        " \n",
        "    # Print the data of dictionary\n",
        "    print(\"\\nPeople1:\", data['people1'])\n",
        "    print(\"\\nPeople2:\", data['people2'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbtqu6ZGBxuY"
      },
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotHistory(history):\n",
        "    metrics = ['train_loss', 'train_precision', 'train_recall', 'train_f1score',\n",
        "           'test_loss','test_precision','test_recall', 'test_f1score']\n",
        "    gradients = ['Greys', 'Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
        "                          'YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu',\n",
        "                          'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn']\n",
        "    norm = matplotlib.colors.Normalize(vmin=0, vmax=len(history))\n",
        "    rows = 2\n",
        "    cols = round(len(metrics)/rows)\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(12,8), sharey = True)\n",
        "\n",
        "    for index, metric in enumerate(metrics):\n",
        "        cmap = matplotlib.cm.get_cmap(gradients[index])\n",
        "\n",
        "        ax = axes[index//cols,index%cols]\n",
        "        for i, val in enumerate(history):\n",
        "            ax.plot(val[metric], color=cmap(norm(i)))\n",
        "        ax.set_title(metric)\n",
        "\n",
        "print(\"For Heuristic: \", heuristics[0])\n",
        "plotHistory(all_histories[0])\n",
        "plt.savefig(f'{heuristics[0]}.png', bbox_inches='tight')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAIR-mobBxuY"
      },
      "outputs": [],
      "source": [
        "print(\"For Heuristic: \", heuristics[1])\n",
        "plotHistory(all_histories[1])\n",
        "plt.savefig(f'{heuristics[1]}.png', bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPc7uxGABxuY"
      },
      "outputs": [],
      "source": [
        "print(\"For Heuristic: \", heuristics[2])\n",
        "plotHistory(all_histories[2])\n",
        "plt.savefig(f'{heuristics[2]}.png', bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIHETnbUBxub"
      },
      "outputs": [],
      "source": [
        "print(\"For Heuristic: \", heuristics[3])\n",
        "plotHistory(all_histories[3])\n",
        "plt.savefig(f'{heuristics[3]}.png', bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8NcSeooBxud"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}